<h2 align="center">
<p>Proyecto 2: TokenizaciÃ³n avanzada y compresiÃ³n de vocabulario con BPE </p>
</h2>


## ğŸ“ƒ DescripciÃ³n

ImplementaciÃ³n de un sistema de **tokenizaciÃ³n** basado en **Byte-Pair Encoding (BPE)** que ajusta serÃ¡ dinÃ¡micamente su vocabulario para maximizar la cobertura y reducir los tÃ©rminos fuera de vocabulario (**OOV**), evaluando su impacto en **modelos de lenguaje n-grama y neuronales**

## ğŸ“Š Dataset

Se utilizarÃ¡ el dataset **WikiText-103** de Hugging Face, que consiste en artÃ­culos de Wikipedia en inglÃ©s, para observar el comportamiento de la tokenizaciÃ³n, la cobertura del vocabulario y el rendimiento de los modelos entrenados.

- **Origen**: [WikiText-103 en Hugging Face](https://huggingface.co/datasets/wikitext)

## ğŸ’» Enfoque
El enfoque del proyecto estÃ¡ centrado en:

* Implementar un sistema de BPE para tokenizar texto

* Experimentar con distintos tamaÃ±os de vocabulario

* Aplicar subword regularization mediante muestreo estocÃ¡stico

* Entrenar modelos de lenguaje:

  * n-grama (orden 3â€“5), evaluando perplejidad

  * RNN simple, evaluando convergencia

* Visualizar los embeddings de sub-tokens con PCA o t-SNE

* Analizar los sub-tokens y merges generados para identificar patrones lingÃ¼Ã­sticos

## ğŸ’» Instrucciones para configurar el entorno con Docker

#### 1. Clonar el repositorio  

```
git clone https://github.com/GabrielaC01/nlp-proyecto02.git
```
#### 2. Construir la imagen Docker

```
docker build -t mi-imagen-nlp .
```
#### 3. Ejecutar el contenedor y acceder a JupyterLab
```
sudo docker run -it --rm \
    -p 8888:8888 \
    -v /tu-ruta/nlp-proyecto02/notebooks:/home/jovyan/work \
    mi-imagen-nlp
```

## ğŸ‘©â€ğŸ’» Maintainers
* Gabriela Colque, Github: [GabrielaC16](https://github.com/GabrielaC16/) Email: gabriela.colque.u@uni.pe   
