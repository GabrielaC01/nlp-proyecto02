{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f123504-8b95-4218-97ac-249656b21c30",
   "metadata": {},
   "source": [
    "#### Entrenamiento de Modelo n-grama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cf2bac5-773f-4772-8b34-472d096a07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dedd3a3-743a-4fe5-a5f5-65d97313e830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import log_loss\n",
    "from src.data_loader import cargar_dataset, tokenize_sentences_by_char, cargar_oraciones_limpias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba347ed-84b6-423c-a944-a0c946eefdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████| 10.5k/10.5k [00:00<00:00, 8.42MB/s]\n",
      "Downloading data: 100%|███████████████████████| 722k/722k [00:01<00:00, 608kB/s]\n",
      "Downloading data: 100%|██████████████████████| 156M/156M [00:06<00:00, 25.8MB/s]\n",
      "Downloading data: 100%|██████████████████████| 156M/156M [00:05<00:00, 26.0MB/s]\n",
      "Downloading data: 100%|██████████████████████| 655k/655k [00:00<00:00, 1.53MB/s]\n",
      "Generating test split: 100%|█████| 4358/4358 [00:00<00:00, 173853.44 examples/s]\n",
      "Generating train split: 100%|█| 1801350/1801350 [00:02<00:00, 890633.27 examples\n",
      "Generating validation split: 100%|█| 3760/3760 [00:00<00:00, 428839.78 examples/\n"
     ]
    }
   ],
   "source": [
    "# Cargamos 5 oraciones limpias del dataset WikiText-103\n",
    "sentences = cargar_oraciones_limpias(split=\"train\", num_oraciones=5)\n",
    "\n",
    "# Aplicamos tokenización por caracteres\n",
    "tokenized_sentences_by_char = tokenize_sentences_by_char(sentences)\n",
    "\n",
    "# Clonamos para aplicar BPE \n",
    "bpe_sentences = [sentence[:] for sentence in tokenized_sentences_by_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de337037-d713-48fd-a060-2c015c5b931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular perplejidad\n",
    "def calculate_perplexity(X_ngram):\n",
    "    # Sumar los logs de las probabilidades para cada token en X_ngram\n",
    "    log_perplexity = -np.sum(np.log(X_ngram.sum(axis=1)))/X_ngram.shape[0]\n",
    "    return np.exp(log_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3af462cf-5662-4180-aff6-7d4fb3b6db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un modelo n-grama simple\n",
    "def train_ngram_model(corpus, n=3):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), tokenizer=lambda x: x.split())\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "997fb51a-45e5-42de-9162-a5dacb3471d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplejidad del modelo n-grama: 0.008396794069600123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preparamos los datos tokenizados \n",
    "corpus = [' '.join(sentence) for sentence in tokenized_sentences_by_char]\n",
    "\n",
    "# Entrenamos el modelo n-grama con n=3 (trigramas)\n",
    "vectorizer, X_ngram = train_ngram_model(corpus, n=3)\n",
    "\n",
    "# Calculamos la perplejidad para el modelo n-grama\n",
    "perplexity_ngram = calculate_perplexity(X_ngram)\n",
    "print(f\"Perplejidad del modelo n-grama: {perplexity_ngram}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ebce4-67de-4b8e-beda-e480e09a1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_ngram = calculate_perplexity(X_ngram)\n",
    "print(f\"Perplejidad del modelo n-grama: {perplexity_ngram}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
