{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7cf2bac5-773f-4772-8b34-472d096a07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9dedd3a3-743a-4fe5-a5f5-65d97313e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from src.data_loader import cargar_oraciones_limpias, tokenize_sentences_by_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fba347ed-84b6-423c-a944-a0c946eefdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo RNN\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Capa LSTM\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x)\n",
    "        # Usamos la última salida del RNN\n",
    "        out = self.fc(out.reshape(-1, out.size(2)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "28f12a30-2bf4-4191-82e4-f020d76a9177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos (convertir oraciones a índices de vocabulario)\n",
    "sentences = cargar_oraciones_limpias(split=\"train\", num_oraciones=10)\n",
    "tokenized_sentences_by_char = tokenize_sentences_by_char(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fce218b8-72d8-4ce2-8677-f13af36efe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vocabulario\n",
    "vocab = list(set([char for sentence in tokenized_sentences_by_char for char in sentence]))\n",
    "vocab_size = len(vocab)\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b2d12e00-5b26-4e97-914a-357369a474b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las oraciones tokenizadas a índices\n",
    "def encode_sentences(sentences):\n",
    "    return [[char_to_idx[char] for char in sentence] for sentence in sentences]\n",
    "\n",
    "encoded_sentences = encode_sentences(tokenized_sentences_by_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7b8e9196-b98b-4c7d-92e7-841d74416760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataLoader para el entrenamiento\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sentences, seq_length):\n",
    "        self.sentences = sentences\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        \n",
    "        # Si la oración es más corta que seq_length, rellénala con un valor de padding (por ejemplo, 0)\n",
    "        padding_length = self.seq_length - len(sentence)\n",
    "        \n",
    "        # Aplicamos padding al final de la secuencia si es necesario\n",
    "        if padding_length > 0:\n",
    "            sentence = sentence + [0] * padding_length  # Añadimos padding (0) al final\n",
    "        else:\n",
    "            sentence = sentence[:self.seq_length]  # Si la secuencia es más larga que seq_length, truncamos\n",
    "            \n",
    "        # Convertir a tensor\n",
    "        inputs = torch.tensor(sentence[:self.seq_length])\n",
    "        targets = torch.tensor(sentence[:self.seq_length])\n",
    "        \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9c565062-353b-4b5e-b9a0-f1dcd8249356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la longitud de la secuencia\n",
    "seq_length = 10  # Ajusta esto según lo necesites\n",
    "\n",
    "# Filtrar oraciones que son más cortas que seq_length (si lo deseas)\n",
    "filtered_sentences = [sentence for sentence in encoded_sentences if len(sentence) >= seq_length]\n",
    "\n",
    "# Crear el dataset con las oraciones filtradas\n",
    "dataset = CharDataset(filtered_sentences, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Definir parámetros\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_size = vocab_size  # El tamaño de salida será igual al tamaño del vocabulario\n",
    "epochs = 5\n",
    "\n",
    "# Inicializamos el modelo, el criterio y el optimizador\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_size)\n",
    "\n",
    "# Definir el criterio (CrossEntropyLoss) y el optimizador (Adam)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "df97bd1f-69a1-446f-bdf2-931500efe294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs shape (before flattening): torch.Size([100, 57])\n",
      "Targets shape (before flattening): torch.Size([10, 10])\n",
      "Flattened Outputs shape: torch.Size([100, 57])\n",
      "Flattened Targets shape: torch.Size([100])\n",
      "Epoch 1, Loss: 3.9436659812927246\n",
      "Outputs shape (before flattening): torch.Size([100, 57])\n",
      "Targets shape (before flattening): torch.Size([10, 10])\n",
      "Flattened Outputs shape: torch.Size([100, 57])\n",
      "Flattened Targets shape: torch.Size([100])\n",
      "Epoch 2, Loss: 3.732245683670044\n",
      "Outputs shape (before flattening): torch.Size([100, 57])\n",
      "Targets shape (before flattening): torch.Size([10, 10])\n",
      "Flattened Outputs shape: torch.Size([100, 57])\n",
      "Flattened Targets shape: torch.Size([100])\n",
      "Epoch 3, Loss: 3.524200201034546\n",
      "Outputs shape (before flattening): torch.Size([100, 57])\n",
      "Targets shape (before flattening): torch.Size([10, 10])\n",
      "Flattened Outputs shape: torch.Size([100, 57])\n",
      "Flattened Targets shape: torch.Size([100])\n",
      "Epoch 4, Loss: 3.3202779293060303\n",
      "Outputs shape (before flattening): torch.Size([100, 57])\n",
      "Targets shape (before flattening): torch.Size([10, 10])\n",
      "Flattened Outputs shape: torch.Size([100, 57])\n",
      "Flattened Targets shape: torch.Size([100])\n",
      "Epoch 5, Loss: 3.12119197845459\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "num_epochs = epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Enviar inputs y targets al modelo\n",
    "        outputs = model(inputs)  # El modelo toma los inputs y devuelve las predicciones\n",
    "\n",
    "        # Verificar las formas antes de aplanar\n",
    "        print(f\"Outputs shape (before flattening): {outputs.shape}\")  # Debería ser (batch_size, seq_length, vocab_size)\n",
    "        print(f\"Targets shape (before flattening): {targets.shape}\")  # Debería ser (batch_size, seq_length)\n",
    "\n",
    "        # Aplanar los outputs y targets de manera correcta\n",
    "        batch_size, seq_length = targets.size()  # Esto te da las dimensiones del batch y secuencia\n",
    "        \n",
    "        # Aseguramos que outputs y targets tengan la misma longitud antes de la pérdida\n",
    "        outputs = outputs.view(-1, vocab_size)  # Aplanar a (batch_size * seq_length, vocab_size)\n",
    "        targets = targets.view(-1)  # Aplanar a (batch_size * seq_length)\n",
    "\n",
    "        # Verificar las formas después de aplanar\n",
    "        print(f\"Flattened Outputs shape: {outputs.shape}\")  # Debería ser (batch_size * seq_length, vocab_size)\n",
    "        print(f\"Flattened Targets shape: {targets.shape}\")  # Debería ser (batch_size * seq_length)\n",
    "\n",
    "        # Verificar si los tamaños coinciden\n",
    "        assert outputs.shape[0] == targets.shape[0], f\"Outputs size {outputs.shape[0]} does not match targets size {targets.shape[0]}\"\n",
    "\n",
    "        # Calculamos la pérdida\n",
    "        loss = criterion(outputs, targets)  # Los outputs y targets deben tener el mismo tamaño\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "57cba4ac-3c6f-4050-81e7-ef49b0c9d07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.927612543106079\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en los datos de validación\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Asumiendo que ya tienes un val_loader configurado de manera similar al dataloader de entrenamiento\n",
    "# Puedes llamar a evaluate_model para calcular la pérdida en el conjunto de validación\n",
    "val_loss = evaluate_model(model, dataloader, criterion)\n",
    "print(f\"Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc192c6a-49fc-4271-a2c0-90e66e42430a",
   "metadata": {},
   "source": [
    "##### CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eeeea79f-3595-4ec4-9a6e-1672d817819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos 5 oraciones\n",
    "sentences = cargar_oraciones_limpias(split=\"train\", num_oraciones=1000)\n",
    "\n",
    "# Aplicamos tokenización por caracteres\n",
    "tokenized_sentences_by_char = tokenize_sentences_by_char(sentences)\n",
    "\n",
    "# Clonamos para aplicar BPE \n",
    "bpe_sentences = [sentence[:] for sentence in tokenized_sentences_by_char]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18ecc0-b037-411e-9169-b0f6f4312ba5",
   "metadata": {},
   "source": [
    "##### ENTRENAMIENTO DEL MODELO N-GRAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "29093957-3bc0-4b29-9e75-706fce19adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplejidad del modelo n-grama: 0.0067004516927731215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Función para calcular perplejidad\n",
    "def calculate_perplexity(X_ngram):\n",
    "    # Sumar los log de las probabilidades para cada token en X_ngram\n",
    "    log_perplexity = -np.sum(np.log(X_ngram.sum(axis=1)))/X_ngram.shape[0]\n",
    "    return np.exp(log_perplexity)\n",
    "\n",
    "# Creamos un modelo n-grama simple (trabajando con caracteres)\n",
    "def train_ngram_model(corpus, n):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), tokenizer=lambda x: x.split())\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, X\n",
    "\n",
    "\n",
    "# Preparamos los datos tokenizados por caracteres\n",
    "corpus = [' '.join(sentence) for sentence in tokenized_sentences_by_char]\n",
    "\n",
    "# Entrenamos el modelo n-grama con bigramas\n",
    "vectorizer, X_ngram = train_ngram_model(corpus, 2)\n",
    "\n",
    "# Calculamos la perplejidad para el modelo n-grama\n",
    "perplexity_ngram = calculate_perplexity(X_ngram)\n",
    "print(f\"Perplejidad del modelo n-grama: {perplexity_ngram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434dcdcf-317c-46f7-adf4-71bea526888e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
